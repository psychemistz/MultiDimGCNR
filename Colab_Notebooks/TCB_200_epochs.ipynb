{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TCB_200_epochs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLYqid7k2hfP",
        "outputId": "056fa20f-1856-4a35-ae5b-d2c4b78ec08e"
      },
      "source": [
        "## Load useful packages\n",
        "!pip install wget\n",
        "\n",
        "from random import sample\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from os import path\n",
        "import h5py\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import wget\n",
        "import cv2\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_sample_image\n",
        "from sklearn.feature_extraction import image\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# import gc\n",
        "from scipy.io import savemat\n",
        "# from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=598fa9e7d270f1b08f204c09174d6c1632c491b27596a50496330538fdc48c55\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCPrr3rj2ySI",
        "outputId": "65151df9-6578-4db5-9b0a-518f06c8ae80"
      },
      "source": [
        "classes = ('cloudy','rain','shine','sunrise')\n",
        "data_path = 'https://raw.githubusercontent.com/Shujaat123/Weather_Classification/master/dataset/'\n",
        "\n",
        "flist = []\n",
        "for fname in classes:\n",
        "  filename = 'WeatherClassificationDB_'+fname+'.mat'\n",
        "  if(path.exists(filename)):\n",
        "    !rm $filename\n",
        "    print('existing file:', filename, ' has been deleted')\n",
        "  print('downloading latest version of file:', filename)\n",
        "  file_path = data_path + filename\n",
        "  wget.download(file_path, filename)\n",
        "  print('DONE')\n",
        "  flist.append(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading latest version of file: WeatherClassificationDB_cloudy.mat\n",
            "DONE\n",
            "downloading latest version of file: WeatherClassificationDB_rain.mat\n",
            "DONE\n",
            "downloading latest version of file: WeatherClassificationDB_shine.mat\n",
            "DONE\n",
            "downloading latest version of file: WeatherClassificationDB_sunrise.mat\n",
            "DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z2-6UM1200q"
      },
      "source": [
        "cloudy_imgs = h5py.File(flist[0], 'r')['images']['input']\n",
        "cloudy_labels = h5py.File(flist[0], 'r')['images']['label']\n",
        "\n",
        "rain_imgs = h5py.File(flist[1], 'r')['images']['input']\n",
        "rain_labels = h5py.File(flist[1], 'r')['images']['label']\n",
        "\n",
        "shine_imgs = h5py.File(flist[2], 'r')['images']['input']\n",
        "shine_labels = h5py.File(flist[2], 'r')['images']['label']\n",
        "\n",
        "sunrise_imgs = h5py.File(flist[3], 'r')['images']['input']\n",
        "sunrise_labels = h5py.File(flist[3], 'r')['images']['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AIbV8n323TI",
        "outputId": "5ef8b24c-46cb-47ca-951e-3e5f020abeb8"
      },
      "source": [
        "print(cloudy_imgs.shape) # (NHWC)\n",
        "print(cloudy_labels.shape)\n",
        "\n",
        "print(rain_imgs.shape) # (NHWC)\n",
        "print(rain_labels.shape)\n",
        "\n",
        "print(shine_imgs.shape) # (NHWC)\n",
        "print(shine_labels.shape)\n",
        "\n",
        "print(sunrise_imgs.shape) # (NHWC)\n",
        "print(sunrise_labels.shape)\n",
        "\n",
        "\n",
        "InputImages = np.concatenate((cloudy_imgs,rain_imgs,shine_imgs,sunrise_imgs), axis = 0)\n",
        "InputImages = InputImages/InputImages.max()\n",
        "ClassLabels = np.concatenate((cloudy_labels,rain_labels,shine_labels,sunrise_labels), axis = 0)\n",
        "\n",
        "InputImages.shape\n",
        "ClassLabels.shape\n",
        "ClassLabels = to_categorical(np.squeeze(ClassLabels)-1)\n",
        "ClassLabels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(207, 256, 256, 3)\n",
            "(207, 1, 1, 1)\n",
            "(215, 256, 256, 3)\n",
            "(215, 1, 1, 1)\n",
            "(253, 256, 256, 3)\n",
            "(253, 1, 1, 1)\n",
            "(357, 256, 256, 3)\n",
            "(357, 1, 1, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1032, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqGtqngx25a7",
        "outputId": "2ffb279d-0b20-4924-c1f9-51ac17f66121"
      },
      "source": [
        "cloudy_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==0)).flatten())\n",
        "rain_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==1)).flatten())\n",
        "shine_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==2)).flatten())\n",
        "sunrise_list = list(np.asarray(np.where(ClassLabels.argmax(axis=1)==3)).flatten())\n",
        "total_list = cloudy_list + rain_list + shine_list + sunrise_list\n",
        "\n",
        "print('Number of \\'cloudy\\' samples:',len(cloudy_list))\n",
        "print('Number of \\'rain\\' samples:',len(rain_list))\n",
        "print('Number of \\'shine\\' samples:',len(shine_list))\n",
        "print('Number of \\'sunrise\\' samples:',len(sunrise_list))\n",
        "print('Total number of samples:',len(total_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of 'cloudy' samples: 207\n",
            "Number of 'rain' samples: 215\n",
            "Number of 'shine' samples: 253\n",
            "Number of 'sunrise' samples: 357\n",
            "Total number of samples: 1032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOlmEhpp27tf"
      },
      "source": [
        "cloudy_train = cloudy_list[0:150]\n",
        "rain_train = rain_list[0:150]\n",
        "shine_train = shine_list[0:150]\n",
        "sunrise_train = sunrise_list[0:150]\n",
        "\n",
        "train_list = cloudy_train + rain_train + shine_train + sunrise_train\n",
        "\n",
        "Input_train = InputImages[train_list]\n",
        "Label_train = ClassLabels[train_list]\n",
        "\n",
        "# valid select 20 cloudy, 20 rain, 20 shine and 20 sunrise samples\n",
        "cloudy_val = cloudy_list[150:170]\n",
        "rain_val = rain_list[150:170]\n",
        "shine_val = shine_list[150:170]\n",
        "sunrise_val = sunrise_list[150:170]\n",
        "val_list = cloudy_val + rain_val + shine_val + sunrise_val\n",
        "\n",
        "Input_val = InputImages[val_list]\n",
        "Label_val = ClassLabels[val_list]\n",
        "\n",
        "## test\n",
        "test_list = list(set(total_list) - set(train_list) - set(val_list))\n",
        "\n",
        "# test_list\n",
        "Input_test = InputImages[test_list]\n",
        "Label_test = ClassLabels[test_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emkmQJ53GZ8n"
      },
      "source": [
        "print(Input_train.shape)\n",
        "temp = np.empty((1,64,64,3))\n",
        "# temp2 = np.empty((1,64,64,3))\n",
        "for i in range(0, Input_train.shape[0]):\n",
        "  patches = image.extract_patches_2d(Input_train[i], (64, 64), random_state=40, max_patches=16)\n",
        "  # noisy = patches + 0.175 * np.random.normal(0, 1, size= patches.shape)\n",
        "  temp = np.concatenate((temp,patches), axis = 0)\n",
        "  # temp2 = np.concatenate((temp2,noisy), axis = 0)\n",
        "y_train = np.asarray(temp[1:])\n",
        "\n",
        "x_train = y_train + 0.1 * np.random.normal(0, 1, size= y_train.shape)\n",
        "x_train = x_train/np.max(np.abs(x_train))\n",
        "x_train = np.clip(x_train, 0., 1.)\n",
        "print(x_train.shape, y_train.shape)\n",
        "\n",
        "\n",
        "y_val = Input_val\n",
        "x_val = y_val + 0.1 * np.random.normal(0, 1, size= y_val.shape)\n",
        "x_val = x_val/np.max(np.abs(x_val))\n",
        "x_val = np.clip(x_val, 0., 1.)\n",
        "\n",
        "print(y_val.shape, x_val.shape)\n",
        "\n",
        "y_test = Input_test\n",
        "x_test = y_test + 0.1 * np.random.normal(0, 1, size= y_test.shape)\n",
        "x_test = x_test/np.max(np.abs(x_test))\n",
        "x_test = np.clip(x_test, 0., 1.)\n",
        "print(y_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfoXNToV49DR"
      },
      "source": [
        "def load_data(Input_train, Input_val, Input_test):\n",
        "  temp = np.empty((1,64,64,3))\n",
        "\n",
        "  for i in range(0, Input_train.shape[0]):\n",
        "    patches = image.extract_patches_2d(Input_train[i], (64, 64), random_state=40, max_patches=16)\n",
        "    temp = np.concatenate((temp,patches), axis = 0)\n",
        "  y_train = np.asarray(temp[1:])\n",
        "\n",
        "  x_train = y_train + 0.1 * np.random.normal(0, 1, size= y_train.shape)\n",
        "  x_train = x_train/np.max(np.abs(x_train))\n",
        "  x_train = np.clip(x_train, 0., 1.)\n",
        "  # print(x_train.shape, y_train.shape)\n",
        "\n",
        "\n",
        "  y_val = Input_val\n",
        "  x_val = y_val + 0.1 * np.random.normal(0, 1, size= y_val.shape)\n",
        "  x_val = x_val/np.max(np.abs(x_val))\n",
        "  x_val = np.clip(x_val, 0., 1.)\n",
        "\n",
        "  # print(y_val.shape, x_val.shape)\n",
        "\n",
        "  y_test = Input_test\n",
        "  x_test = y_test + 0.1 * np.random.normal(0, 1, size= y_test.shape)\n",
        "  x_test = x_test/np.max(np.abs(x_test))\n",
        "  x_test = np.clip(x_test, 0., 1.)\n",
        "  # print(y_test.shape, y_test.shape)\n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySia7ERz3YUn"
      },
      "source": [
        "def RB(input_rb1, num_filter = 64, kernel_size = 3):\n",
        "  x = Conv2D(num_filter, kernel_size, padding='same')(input_rb1)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation(activation='relu')(x)\n",
        "  x = Conv2D(num_filter, kernel_size, padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Add()([input_rb1, x])\n",
        "  y = Activation(activation='relu')(x)\n",
        "  return y\n",
        "\n",
        "def MDRB(MDRB_input, num_filter = 64, kernel_size = 3, D1 = (1,1), D2 = (2,2), D3 = (3,3)):\n",
        "  x = Conv2D(num_filter, kernel_size, padding = 'same')(MDRB_input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation(activation='relu')(x)\n",
        "  xd1_1 = Conv2D(num_filter, kernel_size, padding = 'same', dilation_rate= D1)(x)\n",
        "  xd2_1 = Conv2D(num_filter, kernel_size, padding = 'same', dilation_rate=D2)(x)\n",
        "  xd3_1 = Conv2D(num_filter, kernel_size, padding = 'same', dilation_rate=D3)(x)\n",
        "\n",
        "  xd1_2 = Conv2D(num_filter, kernel_size, padding = 'same', dilation_rate=D1)(xd1_1)\n",
        "  xd2_2 = Conv2D(num_filter, kernel_size, padding = 'same', dilation_rate=D2)(xd2_1)\n",
        "  xd3_2 = Conv2D(num_filter, kernel_size, padding = 'same', dilation_rate=D3)(xd3_1)\n",
        "\n",
        "  x = Concatenate()([xd1_2, xd2_2, xd3_2])\n",
        "  x = Conv2D(num_filter, (1,1), padding = 'same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Add()([x, MDRB_input])\n",
        "  y = Activation(activation='relu')(x)\n",
        "  return y\n",
        "\n",
        "def NMB(input_img, num_filter = 64, kernel_size = 3, num_RB = 8):\n",
        "  x = Conv2D(num_filter, kernel_size, padding='same')(input_img)\n",
        "  for i in range(0, num_RB):\n",
        "    x = RB(x, num_filter, kernel_size)\n",
        "  y = Conv2D(input_img.shape[3], kernel_size, padding='same')(x)\n",
        "  return y\n",
        "\n",
        "def TCB(input_img, output_nmb, num_filter = 64, kernel_size = 3, D1 = (1,1), D2 = (2,2), D3 = (3,3), num_MDRB = 8):\n",
        "  x = Concatenate()([input_img, output_nmb])\n",
        "  x = Conv2D(num_filter, kernel_size, padding = 'same')(x)\n",
        "  for i in range(0, num_MDRB):\n",
        "    x - MDRB(x, num_filter, kernel_size, D1, D2, D3)\n",
        "  y = Conv2D(input_img.shape[3], kernel_size, padding = 'same')(x)\n",
        "  return y\n",
        "\n",
        "def NMB_TCB():\n",
        "  num_filter = 64\n",
        "  kernel_size = 3\n",
        "  strides = 1\n",
        "  num_RB = 8\n",
        "  D1 = (1,1)\n",
        "  D2 = (2,2)\n",
        "  D3 = (3,3)\n",
        "  num_MDRB = 8\n",
        "  lambda_1 = 0.1\n",
        "  lambda_2 = 0.9\n",
        "  \n",
        "  # input_img = Input(shape = (256, 256, 3), name='input_img')\n",
        "  input_img = Input(shape = (None, None, 3), name='input_img')\n",
        "  \n",
        "  output_NMB = NMB(input_img, num_filter, kernel_size, num_RB)\n",
        "  output_NMB = Add(name = 'output_NMB')([input_img, output_NMB])\n",
        "\n",
        "  output_TCB = TCB(input_img, output_NMB, num_filter, kernel_size, D1 , D2, D3, num_MDRB)\n",
        "  \n",
        "  output_CB = Add(name = 'output_CB')([output_NMB, output_TCB])\n",
        "\n",
        "  model = Model(inputs=[input_img], outputs=[output_CB, output_NMB]) \n",
        "  model.compile(optimizer=Adam(learning_rate = 1e-4, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08),\n",
        "                  loss={ 'output_CB': 'mean_squared_error', 'output_NMB': 'mean_squared_error'},\n",
        "                  loss_weights={'output_NMB': lambda_1, 'output_CB': lambda_2}\n",
        "                )\n",
        "\n",
        "  return model\n",
        "\n",
        "model = NMB_TCB()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g7pj_NE8-nS"
      },
      "source": [
        "# DATA_PATH should be changed with the location where you save the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jHkXVYu4gZP"
      },
      "source": [
        "num_trials = 3\n",
        "\n",
        "\n",
        "for loop_ind in range(0, num_trials):\n",
        "  x_train, y_train, x_val, y_val, x_test, y_test = load_data(Input_train, Input_val, Input_test)\n",
        "  \n",
        "  model = NMB_TCB()\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, verbose=0)\n",
        "  \n",
        "  checkpoint = ModelCheckpoint('models\\\\model-best.h5',\n",
        "                                    verbose=1, monitor='val_loss',save_best_only=True, mode='auto', verbose=0)\n",
        "  history = model.fit(x_train, y_train,\n",
        "                        validation_data = (x_val, y_val),\n",
        "                        epochs=200, batch_size=64, callbacks=[checkpoint, es], verbose=0)\n",
        "  \n",
        "  del model\n",
        "  model = keras.models.load_model('models\\\\model-best.h5')\n",
        "\n",
        "  test_pred, a = model.predict(x_test)\n",
        "  test_pred = np.clip(test_pred, 0., 1.)\n",
        "  \n",
        "  save_path = DATA_PATH + 'TCB_NMB_200_epochs/trial_' + str(loop_ind)\n",
        "  if (not(os.path.exists(save_path))):\n",
        "    os.makedirs(save_path)\n",
        "  save_path_target = save_path + '/Target.mat'\n",
        "  save_path_input = save_path + '/Input.mat'\n",
        "  save_path_pred = save_path + '/Predicted.mat'\n",
        "  # print(save_path)\n",
        "  sio.savemat(save_path_target, {'Target':y_test})\n",
        "  sio.savemat(save_path_target, {'Input':x_test})\n",
        "  sio.savemat(save_path_predicted, {'Predicted':test_pred})\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}